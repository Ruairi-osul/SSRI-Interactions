{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\ssri\\lib\\site-packages\\outdated\\utils.py:14: OutdatedPackageWarning: The package outdated is out of date. Your version is 0.2.1, the latest is 0.2.2.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ssri_interactions.config import Config\n",
    "from ssri_interactions.interactions.popcup import PopulationCoupling\n",
    "from ssri_interactions.interactions.preprocessors import InteractionsPreprocessor\n",
    "from ssri_interactions.interactions.loaders import SpontaneousActivityLoader\n",
    "from ssri_interactions.interactions.pairwise import PairwiseCorr, PairwisePartialCorr\n",
    "from ssri_interactions.interactions.runners import InteractionsRunner\n",
    "from ssri_interactions.io import load_derived_generic\n",
    "from ssri_interactions.config import Config\n",
    "import numpy as np\n",
    "\n",
    "dd = Config.derived_data_dir / \"corrs\"\n",
    "dd.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssri_interactions.transforms.graph import GraphTransformer\n",
    "from ssri_interactions.io import load_distances\n",
    "\n",
    "\n",
    "def add_meta_pairwise(df_edge, comb_col=\"comb\", value_coll=\"corr\"):\n",
    "    neuron_types = load_derived_generic(\"neuron_types.csv\")\n",
    "    df_rsc = df_edge.rename(columns={comb_col: \"comb_id\"}).copy()\n",
    "    df_distance = load_distances().rename(\n",
    "        columns=dict(neuron1=\"neuron_1\", neuron2=\"neuron_2\")\n",
    "    )\n",
    "    g_transformer = GraphTransformer(\n",
    "        relabel_nodes=True,\n",
    "        weight_attr=value_coll,\n",
    "        neuron_type_col=\"neuron_type\",\n",
    "        distance_source_col=\"neuron_1\",\n",
    "        distance_target_col=\"neuron_2\",\n",
    "        df_distance=df_distance,\n",
    "    )\n",
    "    df_distance = g_transformer._add_id_comb_col(\n",
    "        df_distance,\n",
    "        source_col=\"neuron_1\",\n",
    "        target_col=\"neuron_2\",\n",
    "        comb_col_name=\"comb_id\",\n",
    "    )\n",
    "    df_rsc = g_transformer._add_id_comb_col(\n",
    "        df_rsc,\n",
    "        source_col=\"neuron_1\",\n",
    "        target_col=\"neuron_2\",\n",
    "        comb_col_name=\"comb_id\",\n",
    "    )\n",
    "    df_rsc = g_transformer._add_neuron_type_comb_col(\n",
    "        df_rsc,\n",
    "        df_neuron_types=neuron_types,\n",
    "        source_col=\"neuron_1\",\n",
    "        target_col=\"neuron_2\",\n",
    "    )\n",
    "    thresh = (\n",
    "        df_rsc.query(\"shuffle==True\")\n",
    "        .groupby([\"nt_comb\", \"bin_width\"])[value_coll]\n",
    "        .apply(lambda x: np.quantile(a=x.dropna().abs(), q=0.99))\n",
    "        .to_dict()\n",
    "    )\n",
    "    df_rsc = (\n",
    "        df_rsc.groupby([\"nt_comb\", \"bin_width\"])\n",
    "        .apply(lambda x: thresh[x.name])\n",
    "        .to_frame(\"thresh\")\n",
    "        .reset_index()\n",
    "        .merge(df_rsc)\n",
    "        .assign(sig=lambda x: x[value_coll].abs() > x[\"thresh\"])\n",
    "    )\n",
    "    return df_rsc.merge(df_distance[[\"comb_id\", \"distance\"]]).merge(\n",
    "        neuron_types[[\"session_name\", \"group\"]].drop_duplicates()\n",
    "    )\n",
    "\n",
    "\n",
    "def add_meta_single(df_node):\n",
    "    neuron_types = load_derived_generic(\"neuron_types.csv\")\n",
    "    df_node = df_node.merge(\n",
    "        neuron_types[\n",
    "            [\"neuron_id\", \"session_name\", \"neuron_type\", \"group\"]\n",
    "        ].drop_duplicates()\n",
    "    )\n",
    "    thresh = (\n",
    "        df_node.query(\"shuffle==True\")\n",
    "        .groupby([\"neuron_type\", \"bin_width\"])[\"cc\"]\n",
    "        .apply(lambda x: np.quantile(a=x.dropna().abs(), q=0.95))\n",
    "        .to_dict()\n",
    "    )\n",
    "    df_node = (\n",
    "        df_node.groupby([\"neuron_type\", \"bin_width\"])\n",
    "        .apply(lambda x: thresh[x.name])\n",
    "        .to_frame(\"thresh\")\n",
    "        .reset_index()\n",
    "        .merge(df_node)\n",
    "        .assign(sig=lambda x: x[\"cc\"].abs() > x[\"thresh\"])\n",
    "    )\n",
    "    return df_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pairwise Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_types = load_derived_generic(\"neuron_types.csv\")\n",
    "sessions = neuron_types[\"session_name\"].dropna().unique()\n",
    "\n",
    "res_corr = []\n",
    "res_pcorr = []\n",
    "res_pcup = []\n",
    "bin_widths = [0.05, 0.1, 0.5, 1]\n",
    "\n",
    "loader_fac = lambda bin_width, shuffle: SpontaneousActivityLoader(\n",
    "    session_name=sessions[0], bin_width=bin_width, shuffle=shuffle\n",
    ")\n",
    "preprocessor_fac = lambda: InteractionsPreprocessor(\n",
    "    z=True, minmax=False, gaussian_sigma=None\n",
    ")\n",
    "corr_fac = lambda: PairwiseCorr()\n",
    "pcorr_fac = lambda: PairwisePartialCorr()\n",
    "pcup_fac = lambda: PopulationCoupling()\n",
    "\n",
    "runner_fac = lambda bin_width, shuffle: InteractionsRunner(\n",
    "    loader=loader_fac(bin_width, shuffle=shuffle),\n",
    "    preprocessor=preprocessor_fac(),\n",
    "    pcup=pcup_fac(),\n",
    "    corr=corr_fac(),\n",
    "    pcorr=pcorr_fac(),\n",
    ")\n",
    "for shuffle in (True, False):\n",
    "    for bin_width in bin_widths:\n",
    "        runner = runner_fac(bin_width, shuffle)\n",
    "        res_pcorr.append(runner.run_pcorr_multi(sessions).assign(bin_width=bin_width, shuffle=shuffle))\n",
    "        res_corr.append(runner.run_corr_multi(sessions).assign(bin_width=bin_width, shuffle=shuffle))\n",
    "        res_pcup.append(runner.run_pcup_multi(sessions).assign(bin_width=bin_width, shuffle=shuffle))\n",
    "\n",
    "\n",
    "df_corr = pd.concat(res_corr).reset_index(drop=True).pipe(add_meta_pairwise, value_coll=\"corr\")\n",
    "df_pcorr = pd.concat(res_pcorr).reset_index(drop=True).pipe(add_meta_pairwise, value_coll=\"pcorr\")\n",
    "df_pcup = pd.concat(res_pcup).reset_index(drop=True).pipe(add_meta_single)\n",
    "\n",
    "df_corr.to_parquet(dd / \"spont - corr.parquet\", index=False)\n",
    "df_pcorr.to_parquet(dd / \"spont - pcorr.parquet\", index=False)\n",
    "df_pcup.to_parquet(dd / \"spont - pcup.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ssri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43cbdb6d61dab472ca36965b8e063b8466d44641e4bc0dd430fcfe888de07997"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
